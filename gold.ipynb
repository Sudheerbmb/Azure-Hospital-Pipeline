{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b0ae4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, col, expr, current_timestamp, to_timestamp, sha2, concat_ws, coalesce, monotonically_increasing_id\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import Window\n",
    "\n",
    "#ADLS configuration \n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.<<Storageaccount_name>>.dfs.core.windows.net\",\n",
    "  \"<<Storage_Account_access_key>>\"\n",
    ")\n",
    "\n",
    "\n",
    "# Paths\n",
    "silver_path = \"abfss://<<container>>@<<Storageaccount_name>>.core.windows.net/<<path>>\"\n",
    "gold_dim_patient = \"abfss://<<container>>@<<Storageaccount_name>>.core.windows.net/<<path>>\"\n",
    "gold_dim_department = \"abfss://<<container>>@<<Storageaccount_name>>.core.windows.net/<<path>>\"\n",
    "gold_fact = \"abfss://<<container>>@<<Storageaccount_name>>.core.windows.net/<<path>>\"\n",
    "\n",
    "# Read silver data (assume append-only)\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Define window for latest admission per patient\n",
    "w = Window.partitionBy(\"patient_id\").orderBy(F.col(\"admission_time\").desc())\n",
    "\n",
    "silver_df = (\n",
    "    silver_df\n",
    "    .withColumn(\"row_num\", F.row_number().over(w))  # Rank by latest admission_time\n",
    "    .filter(F.col(\"row_num\") == 1)                  # Keep only latest row\n",
    "    .drop(\"row_num\")\n",
    ")\n",
    "\n",
    "#Patient Dimension Table Creation\n",
    "# Prepare incoming dimension records (deduplicated per patient, latest record)\n",
    "incoming_patient = (silver_df\n",
    "                    .select(\"patient_id\", \"gender\", \"age\")\n",
    "                    .withColumn(\"effective_from\", current_timestamp())\n",
    "                   )\n",
    "\n",
    "# Create target if not exists\n",
    "if not DeltaTable.isDeltaTable(spark, gold_dim_patient):\n",
    "    # initialize table with schema and empty data\n",
    "    incoming_patient.withColumn(\"surrogate_key\", F.monotonically_increasing_id()) \\\n",
    "                    .withColumn(\"effective_to\", lit(None).cast(\"timestamp\")) \\\n",
    "                    .withColumn(\"is_current\", lit(True)) \\\n",
    "                    .write.format(\"delta\").mode(\"overwrite\").save(gold_dim_patient)\n",
    "\n",
    "# Load target as DeltaTable\n",
    "target_patient = DeltaTable.forPath(spark, gold_dim_patient)\n",
    "\n",
    "# Create an expression to detect attribute changes (hash or explicit comparisons)\n",
    "# We'll use a simple concat hash to detect changes\n",
    "incoming_patient = incoming_patient.withColumn(\n",
    "    \"_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\", F.coalesce(col(\"gender\"), lit(\"NA\")), F.coalesce(col(\"age\").cast(\"string\"), lit(\"NA\"))), 256)\n",
    ")\n",
    "\n",
    "# Bring target current hash\n",
    "target_patient_df = spark.read.format(\"delta\").load(gold_dim_patient).withColumn(\n",
    "    \"_target_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\", F.coalesce(col(\"gender\"), lit(\"NA\")), F.coalesce(col(\"age\").cast(\"string\"), lit(\"NA\"))), 256)\n",
    ").select(\"surrogate_key\", \"patient_id\", \"gender\", \"age\", \"is_current\", \"_target_hash\", \"effective_from\", \"effective_to\")\n",
    "\n",
    "# Create temp views for merge\n",
    "incoming_patient.createOrReplaceTempView(\"incoming_patient_tmp\")\n",
    "target_patient_df.createOrReplaceTempView(\"target_patient_tmp\")\n",
    "\n",
    "# We'll implement in two steps using Delta MERGE (safe & explicit)\n",
    "\n",
    "# 1) Mark old current rows as not current where changed\n",
    "changes_df = spark.sql(\"\"\"\n",
    "SELECT t.surrogate_key, t.patient_id\n",
    "FROM target_patient_tmp t\n",
    "JOIN incoming_patient_tmp i\n",
    "  ON t.patient_id = i.patient_id\n",
    "WHERE t.is_current = true AND t._target_hash <> i._hash\n",
    "\"\"\")\n",
    "\n",
    "changed_keys = [row['surrogate_key'] for row in changes_df.collect()]\n",
    "\n",
    "if changed_keys:\n",
    "    # Update existing current records: set is_current=false and effective_to=current_timestamp()\n",
    "    target_patient.update(\n",
    "        condition = expr(\"is_current = true AND surrogate_key IN ({})\".format(\",\".join([str(k) for k in changed_keys]))),\n",
    "        set = {\n",
    "            \"is_current\": expr(\"false\"),\n",
    "            \"effective_to\": expr(\"current_timestamp()\")\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 2) Insert new rows for changed & new records\n",
    "# Build insert DF: join incoming with target to figure new inserts where either not exists or changed\n",
    "inserts_df = spark.sql(\"\"\"\n",
    "SELECT i.patient_id, i.gender, i.age, i.effective_from, i._hash\n",
    "FROM incoming_patient_tmp i\n",
    "LEFT JOIN target_patient_tmp t\n",
    "  ON i.patient_id = t.patient_id AND t.is_current = true\n",
    "WHERE t.patient_id IS NULL OR t._target_hash <> i._hash\n",
    "\"\"\").withColumn(\"surrogate_key\", F.monotonically_increasing_id()) \\\n",
    "  .withColumn(\"effective_to\", lit(None).cast(\"timestamp\")) \\\n",
    "  .withColumn(\"is_current\", lit(True)) \\\n",
    "  .select(\"surrogate_key\", \"patient_id\", \"gender\", \"age\", \"effective_from\", \"effective_to\", \"is_current\")\n",
    "\n",
    "# Append new rows\n",
    "if inserts_df.count() > 0:\n",
    "    inserts_df.write.format(\"delta\").mode(\"append\").save(gold_dim_patient)\n",
    "\n",
    "\n",
    "\n",
    "# Department Dimension Table Creation\n",
    "\n",
    "# prepare incoming (latest per patient feed snapshot)\n",
    "incoming_dept = (silver_df\n",
    "                 .select(\"department\", \"hospital_id\")\n",
    "                )\n",
    "\n",
    "# add hash and dedupe incoming (one row per natural key)\n",
    "incoming_dept = incoming_dept.dropDuplicates([\"department\", \"hospital_id\"]) \\\n",
    "    .withColumn(\"surrogate_key\", monotonically_increasing_id())\n",
    "\n",
    "# initialize table if missing\n",
    "\n",
    "incoming_dept.select(\"surrogate_key\", \"department\", \"hospital_id\") \\\n",
    "    .write.format(\"delta\").mode(\"overwrite\").save(gold_dim_department)\n",
    "\n",
    "\n",
    "\n",
    "# Create Fact table\n",
    "\n",
    "# Read current dims (filter is_current=true)\n",
    "dim_patient_df = (spark.read.format(\"delta\").load(gold_dim_patient)\n",
    "                  .filter(col(\"is_current\") == True)\n",
    "                  .select(col(\"surrogate_key\").alias(\"surrogate_key_patient\"), \"patient_id\", \"gender\", \"age\"))\n",
    "\n",
    "dim_dept_df = (spark.read.format(\"delta\").load(gold_dim_department)\n",
    "               .select(col(\"surrogate_key\").alias(\"surrogate_key_dept\"), \"department\", \"hospital_id\"))\n",
    "\n",
    "# Build base fact from silver events\n",
    "fact_base = (silver_df\n",
    "             .select(\"patient_id\", \"department\", \"hospital_id\", \"admission_time\", \"discharge_time\", \"bed_id\")\n",
    "             .withColumn(\"admission_date\", F.to_date(\"admission_time\"))\n",
    "            )\n",
    "\n",
    "# Join to get surrogate keys\n",
    "fact_enriched = (fact_base\n",
    "                 .join(dim_patient_df, on=\"patient_id\", how=\"left\")\n",
    "                 .join(dim_dept_df, on=[\"department\", \"hospital_id\"], how=\"left\")\n",
    "                )\n",
    "\n",
    "# Compute metrics\n",
    "fact_enriched = fact_enriched.withColumn(\"length_of_stay_hours\",\n",
    "                                         (F.unix_timestamp(col(\"discharge_time\")) - F.unix_timestamp(col(\"admission_time\"))) / 3600.0) \\\n",
    "                             .withColumn(\"is_currently_admitted\", F.when(col(\"discharge_time\") > current_timestamp(), lit(True)).otherwise(lit(False))) \\\n",
    "                             .withColumn(\"event_ingestion_time\", current_timestamp())\n",
    "\n",
    "# Let's make column names explicit instead:\n",
    "fact_final = fact_enriched.select(\n",
    "    F.monotonically_increasing_id().alias(\"fact_id\"),\n",
    "    col(\"surrogate_key_patient\").alias(\"patient_sk\"),\n",
    "    col(\"surrogate_key_dept\").alias(\"department_sk\"),\n",
    "    \"admission_time\",\n",
    "    \"discharge_time\",\n",
    "    \"admission_date\",\n",
    "    \"length_of_stay_hours\",\n",
    "    \"is_currently_admitted\",\n",
    "    \"bed_id\",\n",
    "    \"event_ingestion_time\"\n",
    ")\n",
    "\n",
    "# Persist fact table partitioned by admission_date (helps Synapse / queries)\n",
    "fact_final.write.format(\"delta\").mode(\"overwrite\").save(gold_fact)\n",
    "\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"Patient dim count:\", spark.read.format(\"delta\").load(gold_dim_patient).count())\n",
    "print(\"Department dim count:\", spark.read.format(\"delta\").load(gold_dim_department).count())\n",
    "print(\"Fact rows:\", spark.read.format(\"delta\").load(gold_fact).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f80ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#ADLS configuration \n",
    "\n",
    "\n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.hospitalstoraage.dfs.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = \"hospitalanalyticsvaultscope\", key = \"storage-connection\")\n",
    "\n",
    ")\n",
    "bronze_path = \"abfss://bronze@hospitalstoraage.dfs.core.windows.net/patient_flow\"\n",
    "silver_path = \"abfss://silver@hospitalstoraage.dfs.core.windows.net/patient_flow\"\n",
    "\n",
    "#read from bronze\n",
    "bronze_df = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(bronze_path)\n",
    ")\n",
    "\n",
    "#Defin Schema\n",
    "schema = StructType([\n",
    "    StructField(\"patient_id\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"department\", StringType()),\n",
    "    StructField(\"admission_time\", StringType()),\n",
    "    StructField(\"discharge_time\", StringType()),\n",
    "    StructField(\"bed_id\", IntegerType()),\n",
    "    StructField(\"hospital_id\", IntegerType())\n",
    "])\n",
    "\n",
    "#Parse it to dataframe\n",
    "parsed_df = bronze_df.withColumn(\"data\",from_json(col(\"raw_json\"),schema)).select(\"data.*\")\n",
    "\n",
    "#convert type to Timestamp\n",
    "clean_df = parsed_df.withColumn(\"admission_time\", to_timestamp(\"admission_time\"))\n",
    "clean_df = clean_df.withColumn(\"discharge_time\", to_timestamp(\"discharge_time\"))\n",
    "\n",
    "#invalid admission_times\n",
    "clean_df = clean_df.withColumn(\"admission_time\",\n",
    "                               when(\n",
    "                                   col(\"admission_time\").isNull() | (col(\"admission_time\") > current_timestamp()),\n",
    "                                   current_timestamp())\n",
    "                               .otherwise(col(\"admission_time\")))\n",
    "\n",
    "#Handle Invalid Age\n",
    "clean_df = clean_df.withColumn(\"age\",\n",
    "                               when(col(\"age\")>100,floor(rand()*90+1).cast(\"int\"))\n",
    "                               .otherwise(col(\"age\"))\n",
    "                               )\n",
    "\n",
    "#schema evolution\n",
    "expected_cols = [\"patient_id\", \"gender\", \"age\", \"department\", \"admission_time\", \"discharge_time\", \"bed_id\", \"hospital_id\"]\n",
    "\n",
    "for col_name in expected_cols:\n",
    "    if col_name not in clean_df.columns:\n",
    "        clean_df = clean_df.withColumn(col_name, lit(None))\n",
    "\n",
    "#Write to silver table\n",
    "(\n",
    "    clean_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"mergeSchema\",\"true\")\n",
    "    .option(\"checkpointLocation\", silver_path + \"_checkpoint\")\n",
    "    .start(silver_path)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
